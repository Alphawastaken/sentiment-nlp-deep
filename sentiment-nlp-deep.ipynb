{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntwYAZfhPwOo"
      },
      "source": [
        "* gdrive\n",
        "    * MyDrive\n",
        "        * CSVS\n",
        "        * TSVS\n",
        "        * data\n",
        "            * 2019\n",
        "                * apri\n",
        "                    * reviews.csv\n",
        "                * febrouary\n",
        "                    * reviews.csv\n",
        "                * march\n",
        "                    * reviews.csv\n",
        "            * 2023\n",
        "                * june\n",
        "                    * reviews.csv\n",
        "                * march\n",
        "                    * reviews.csv\n",
        "                * september\n",
        "                    * reviews.csv\n",
        "            * negative.txt\n",
        "            * positive.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z0-XnH1n6Pm",
        "outputId": "d2a3f8b2-c2e1-4b47-c312-3641b371ac6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR4DjiFpvCnv",
        "outputId": "deca82f7-599c-48ac-d8a1-aa90b9533066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=9cc0bac0a563809a74f815f90db8371c72deba6a15b49302c7085fc1cfebd58d\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IHyp6LTu3ac"
      },
      "outputs": [],
      "source": [
        "#Ερώτημα 1 (kai gia tis 2 xronies)\n",
        "\n",
        "#Sunartisi (fix_data) pou ftiaxnei mono me sxoleia ena csv kai to apothikeuei\n",
        "#Sunartisi (cleaning) katharizei ton \"thoribo\"\n",
        "#Sunartisi (get_sentiment) gia kathe sxoleio mas epistrefi thn katigoria poy anoikei\n",
        "\n",
        "from langdetect import detect\n",
        "\n",
        "def fix_data(file_path, percentage, months: list):\n",
        "\n",
        "    combined_comments = []  # To store the data\n",
        "    for month in months:\n",
        "        reviews_file_path = file_path + month + \"/reviews.csv\"#get the path\n",
        "        comments = pd.read_csv(reviews_file_path)#read the csv\n",
        "\n",
        "        num_rows_to_keep = percentage\n",
        "        comments = comments.sample(n=num_rows_to_keep)#sample the rows\n",
        "\n",
        "        for column in comments.columns: # Keep only the column \"comments\"\n",
        "            if column != \"comments\":\n",
        "                comments = comments.drop(column, axis=1)#drop it\n",
        "        comments = comments.dropna() #drop nan values\n",
        "        combined_comments.append(comments) #append the df to the list\n",
        "\n",
        "    comments = pd.concat(combined_comments) #concat the dataframe\n",
        "    comments = comments.drop_duplicates()\n",
        "\n",
        "    for comment in comments[\"comments\"]:\n",
        "        try:\n",
        "            detection=detect(comment)#detect the language\n",
        "            if detection != \"en\":#if it is not english\n",
        "                comments = comments.drop(comments[comments[\"comments\"] == comment].index)#drop it\n",
        "        except:\n",
        "            comments = comments.drop(comments[comments[\"comments\"] == comment].index)\n",
        "\n",
        "    if \"2019\" in file_path:#if it is 2019\n",
        "        comments.to_csv(\"/content/gdrive/MyDrive/CSVS/train_2019.csv\", index=False)\n",
        "    else:\n",
        "        comments.to_csv(\"/content/gdrive/MyDrive/CSVS/train_2023.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "def cleaning(text):\n",
        "    text = str(text).lower()#convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)#remove punctuation\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)#remove hashtags and mentions\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)#remove urls\n",
        "    text = re.sub(r'\\d+', '', text)#remove digits\n",
        "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)#remove emojis\n",
        "\n",
        "    stopwords_cleaning = stopwords.words('english')#remove stopwords\n",
        "    text_word = text.split()#split the text\n",
        "    filter_words = [word for word in text_word if word not in stopwords_cleaning]#filter the stopwords\n",
        "    text = ' '.join(filter_words)#join the words\n",
        "\n",
        "    return text#return the cleaned text\n",
        "\n",
        "\n",
        "def get_sentiment(df):\n",
        "    pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")#load the sentiment analysis model\n",
        "    def func(text):#function to get the sentiment\n",
        "        try:#try to get the sentiment\n",
        "            return pipe(text)[0]['label']\n",
        "        except:\n",
        "            return 'error'\n",
        "    df['sentiment'] = df['comments'].apply(func)#apply the function to the comments\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_FpnQo5vVA9"
      },
      "outputs": [],
      "source": [
        "#ftiaxnoyme 2 arxeia CSV me sxoleia gia to 2019,2023\n",
        "file_path_2019 = \"/content/gdrive/MyDrive/data/2019\"\n",
        "file_path_2023 = \"/content/gdrive/MyDrive/data/2023\"\n",
        "\n",
        "months_2019 = ['/april', '/febrouary', '/march']\n",
        "months_2023 = ['/june', '/march', '/september']\n",
        "\n",
        "#auto to komati pernei ligi ora (pano-kato 20 lepta,to trejame polles fores kai den eixame thema me thn ram,ama thelete kante to 15000,8000 antoistixa\n",
        "#prokeimenoy meta thn sunartiti (cook) na exoyme pano apo 450 comments) (me auta poy exoyme parakato exoyme sxedon 1000comments)\n",
        "fix_data(file_path_2019, 20000, months_2019)\n",
        "fix_data(file_path_2023, 10000,months_2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXgOh4hpGENs"
      },
      "outputs": [],
      "source": [
        "#katharizoyme ta CSV pou ftiaksame prin.Fortonoyme to thetiko kai arnitiko synolo lekseon.\n",
        "\n",
        "#Synartisi (cook) sumfona me to silono thetikon kai arntikikon lekseon poy exei mia protasi se sxesi me ayta poy fortosame prin thn katatasoyme se mia apo tis 3 katigories (NEUTRAL,POSITIVE,NEGATIVE).Kai meta moinoyme ligo-ligo kata ena 10% ta thetika sxoleia prokeimenou na ftasoume sto epithimito apotelesema (50%).\n",
        "#Enan antistixo mixanismo efarmozoyme kai gia NEUTRAL alla me orio (30%).Sto telos apla prosuetoume mia akoma stilei opou prosthetoume to ID sto telos apothikeuoume se CSV morfi ena arxeio me (ID,COMMENT,SENTIMENT)\n",
        "\n",
        "train_2019 = pd.read_csv(\"/content/gdrive/MyDrive/CSVS/train_2019.csv\")\n",
        "train_2019['comments']=train_2019['comments'].apply(cleaning)#cleaning the comments\n",
        "\n",
        "train_2023 = pd.read_csv(\"/content/gdrive/MyDrive/CSVS/train_2023.csv\")#read the file\n",
        "train_2023['comments']=train_2023['comments'].apply(cleaning)#cleaning the comments\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/data/positive.txt\", 'r') as file:#open the file\n",
        "    positive_words = set(file.read().splitlines())#read the file and split the lines\n",
        "with open(\"/content/gdrive/MyDrive/data/negative.txt\", 'r') as file:#open the file\n",
        "    negative_words = set(file.read().splitlines())#read the file and split the lines\n",
        "\n",
        "\n",
        "def persentage(x):#function to calculate the percentage\n",
        "        return round((x/size)*100,2)\n",
        "\n",
        "def cook(df,year):\n",
        "    global size\n",
        "\n",
        "    rows=[]#to store the data\n",
        "    for comments in df['comments']:#for each comment\n",
        "        word=set(comments.split())#split the comment\n",
        "        positive_sum = len(word.intersection(positive_words))#find the positive words\n",
        "        negative_sum = len(word.intersection(negative_words))#find the negative words\n",
        "        if positive_sum==negative_sum:\n",
        "            rows.append({'name':comments,'sentiment':'NEUTRAL'})#if the positive and negative words are equal then the sentiment is neutral\n",
        "        elif positive_sum>negative_sum:\n",
        "            rows.append({'name':comments,'sentiment':'POSITIVE'})#if the positive words are more than the negative then the sentiment is positive\n",
        "        else:\n",
        "            rows.append({'name':comments,'sentiment':'NEGATIVE'})#if the negative words are more than the positive then the sentiment is negative\n",
        "\n",
        "    my_df=pd.DataFrame()#create a dataframe\n",
        "    my_df[['comments','sentiment']]=pd.DataFrame(rows)#add the data to the dataframe\n",
        "\n",
        "    size = len(my_df)#get the size of the dataframe\n",
        "\n",
        "    positive_comments = my_df[my_df['sentiment'] == 'POSITIVE'].shape[0]#get the positive comments\n",
        "    neutral_comments = my_df[my_df['sentiment'] == 'NEUTRAL'].shape[0]#get the neutral comments\n",
        "\n",
        "    positive_per = (positive_comments / size) * 100 #calculate the percentage of the positive comments\n",
        "    neutral_per = (neutral_comments / size) * 100   #calculate the percentage of the neutral comments\n",
        "\n",
        "    while positive_per > 50.0 or (neutral_per > 30.0 or neutral_per < 15.0):#while the percentage of the positive comments is more than 50% or the percentage of the neutral comments is more than 30% or less than 15%\n",
        "        if positive_per > 50.0:#if the percentage of the positive comments is more than 50%\n",
        "            my_df = my_df.drop(my_df[my_df['sentiment'] == 'POSITIVE'].sample(n=int(positive_comments * (10 / 100))).index)#drop the 10% of the positive comments\n",
        "            size = len(my_df)\n",
        "\n",
        "        neutral_comments = my_df[my_df['sentiment'] == 'NEUTRAL'].shape[0]#get the neutral comments\n",
        "        neutral_per = (neutral_comments / size) * 100\n",
        "\n",
        "        if neutral_per > 30.0:#if the percentage of the neutral comments is more than 30%\n",
        "            my_df = my_df.drop(my_df[my_df['sentiment'] == 'NEUTRAL'].sample(n=int(neutral_comments * (10 / 100))).index)#drop the 10% of the neutral comments\n",
        "            size = len(my_df)\n",
        "\n",
        "        neutral_comments = my_df[my_df['sentiment'] == 'NEUTRAL'].shape[0]#get the neutral comments\n",
        "        neutral_per = (neutral_comments / size) * 100\n",
        "        positive_comments = my_df[my_df['sentiment'] == 'POSITIVE'].shape[0]#get the positive comments\n",
        "        positive_per = (positive_comments / size) * 100\n",
        "\n",
        "    my_df['id'] = range(1, len(my_df) + 1)#add an id to the dataframe\n",
        "\n",
        "    path = \"/content/gdrive/MyDrive/CSVS/sentiment_\" + str(year) + \".csv\"\n",
        "    my_df.to_csv(path, index=False)\n",
        "\n",
        "cook(train_2019,2019)\n",
        "cook(train_2023,2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "NIDg_4sjqMHn",
        "outputId": "46303ffe-82d9-4b62-ecda-dc0652c3a867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Statistcs 2019 \n",
            "  Sentiment  Count  Perentage\n",
            "0  NEGATIVE    274      20.19\n",
            "1   NEUTRAL    405      29.85\n",
            "2  POSITIVE    678      49.96\n",
            "Statistcs 2023 \n",
            "  Sentiment  Count  Perentage\n",
            "0  NEGATIVE    180      21.23\n",
            "1   NEUTRAL    247      29.13\n",
            "2  POSITIVE    421      49.65\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Ypologizoume ta statistika dedomena ton CSV poy ftiajame parapano + Sugkrinoyme over time allages sta sentiment (kanoyme 1 grafima opoy fenontai oi diagores gia to 2 etoi)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def Statistics(year):\n",
        "    #compute statistics\n",
        "    global size\n",
        "\n",
        "    path = \"/content/gdrive/MyDrive/CSVS/sentiment_\" + str(year) + \".csv\"\n",
        "    my_df = pd.read_csv(path)\n",
        "\n",
        "    my_df = my_df.groupby(\"sentiment\")[\"comments\"].count() #upologizmos tou posa comment exei kathe katigoria (sentiment)\n",
        "    my_df = my_df.reset_index()\n",
        "    my_df.columns = ['Sentiment', 'Count'] #bazoyme tis stiles sto dataframe\n",
        "    size = my_df['Count'].sum() #upologizoyme posa einai ta sunolika Comments\n",
        "    my_df[\"Perentage\"] = my_df[\"Count\"].apply(persentage) #briskoume kai apothikeuoyme tis analogies gia kathe omada sxoleion se mia stilh\n",
        "    return my_df\n",
        "\n",
        "def compare_sentiment_over_time(df1 = Statistics(2019), df2 = Statistics(2023)):#compare the sentiment over time\n",
        "\n",
        "    print(f\"Statistcs 2019 \\n{df1}\")#print the statistics\n",
        "    print(f\"Statistcs 2023 \\n{df2}\")\n",
        "\n",
        "    sentiment_1 = df1['Count']#get the count of the sentiment\n",
        "    sentiment_2 = df2['Count']\n",
        "\n",
        "    labels = df1['Sentiment']#get the labels\n",
        "    x = range(len(labels))#get the range of the labels\n",
        "\n",
        "    # Ensure that both arrays have the same length\n",
        "    min_length = min(len(sentiment_1), len(sentiment_2))#get the min length\n",
        "    sentiment_1 = sentiment_1[:min_length]#get the min length\n",
        "    sentiment_2 = sentiment_2[:min_length]\n",
        "\n",
        "    plt.bar(x, sentiment_1, width=0.4, label='2019')#plot the bar\n",
        "    plt.bar([i + 0.4 for i in x], sentiment_2, width=0.4, label='2023')\n",
        "\n",
        "    plt.xlabel('Sentiment')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Sentiment Distribution Over Time')\n",
        "    plt.xticks([i + 0.2 for i in x], labels)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "compare_sentiment_over_time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-Ee6dHd1SVvr"
      },
      "outputs": [],
      "source": [
        "#Ερώτημα 2 (kai gia tis 2 xronies)\n",
        "\n",
        "#Sunartisi (CSV_To_TSV) pou metatrepei to CSV se TSV kai to apothikeuei\n",
        "#Sunartisi (load_tsv) kanei load to TSV poy theloyme\n",
        "#Sunartisi (split) apla xorizei sta 2 to arxeio (80% train,20% test) kai kanei drop N/A grammes amma uparxoyn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def CSV_To_TSV(year):#convert csv to tsv\n",
        "    path = \"/content/gdrive/MyDrive/CSVS/sentiment_\" + str(year) + \".csv\"#path\n",
        "    my_df = pd.read_csv(path)#read the csv\n",
        "    my_df = my_df.dropna()#drop nan values\n",
        "    path = \"/content/gdrive/MyDrive/TSVS/data_\" + str(year) + \".tsv\"\n",
        "    my_df.to_csv(path, sep='\\t', index=False)#save the tsv\n",
        "\n",
        "CSV_To_TSV(2019)\n",
        "CSV_To_TSV(2023)\n",
        "\n",
        "def load_tsv(year):#load the tsv file\n",
        "    path = \"/content/gdrive/MyDrive/TSVS/data_\" + str(year) + \".tsv\"#path to the file\n",
        "    data_tsv = pd.read_csv(path,sep='\\t')#read the file\n",
        "    return data_tsv\n",
        "\n",
        "def split(year):\n",
        "    my_df = load_tsv(year)#load the tsv file\n",
        "    train_df, test_df = train_test_split(my_df, test_size=0.2, random_state=42)#split the data\n",
        "\n",
        "    train_df = train_df.dropna()#drop the nan values\n",
        "    test_df = test_df.dropna()#drop the nan values\n",
        "\n",
        "    return train_df, test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hvuUffYczRT"
      },
      "outputs": [],
      "source": [
        "#TF-IDF function\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def TFIDF(year):\n",
        "    #load the TSV\n",
        "    train, test = split(year)#split the data\n",
        "\n",
        "    train_sent = train[\"sentiment\"]#get the sentiment\n",
        "    test_sent = test[\"sentiment\"]#get the sentiment\n",
        "\n",
        "    TFIDF_vectorizer = TfidfVectorizer(max_features=1000)#initialize the TF-IDF vectorizer\n",
        "    train_TFIDF = TFIDF_vectorizer.fit_transform(train[\"comments\"]) #train the TF-IDF vectorizer with the train\n",
        "    test_TFIDF = TFIDF_vectorizer.transform(test[\"comments\"])#transform the test data\n",
        "\n",
        "    return train_TFIDF, test_TFIDF, train_sent, test_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qli1BRUGc0bU"
      },
      "outputs": [],
      "source": [
        "#Word Embedding function\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#pernoume kathe leksei se mia protasei prosthetoume ta antoistixa vector toys kai meta breiskoyme to MO prokeimenoy na exoyme sta xeria mas to Value olhs ths Protaseis\n",
        "def get_average_word2vec(tokens_list, model, generate_missing=False, k=100):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    vectorized = [model.wv[word] if word in model.wv else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    text = text.lower().split()\n",
        "    return text\n",
        "\n",
        "#upologizmos toy Word_Embeddings gia ta comments mias xronias\n",
        "def Word_Embeddings(year):\n",
        "\n",
        "    train, test = split(year) #Load the TSV files\n",
        "\n",
        "    train_sent = train[\"sentiment\"]\n",
        "    test_sent = test[\"sentiment\"]\n",
        "\n",
        "    model = Word2Vec(sentences=[word_tokenize(comment) for comment in train['comments']], vector_size=100, window=5, min_count=1, workers=4) #train the model\n",
        "\n",
        "    train_embeddings = np.array([get_average_word2vec(text_to_word_list(comment), model) for comment in train['comments']])\n",
        "    test_embeddings = np.array([get_average_word2vec(text_to_word_list(comment), model) for comment in test['comments']])\n",
        "\n",
        "    return train_embeddings, test_embeddings, train_sent, test_sent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC8Yq996OR7c",
        "outputId": "4e8cb5bd-b939-476f-c4f0-46d13f6fdb2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "resultes for year 2019 with model type (tfidf)\n",
            "{'Precision': 0.7777101029010638, 'Recall': 0.7806150186884132, 'F_Measure': 0.7736917745292613, 'Accuracy': 0.7806150186884132, 'Name': 'SVM'}\n",
            "{'Precision': 0.7580050306400244, 'Recall': 0.7603635745837581, 'F_Measure': 0.7386512010230535, 'Accuracy': 0.7484029901461094, 'Name': 'Random Forest'}\n",
            "{'Precision': 0.4416962564160455, 'Recall': 0.31058443764865784, 'F_Measure': 0.1663696690057182, 'Accuracy': 0.31058443764865784, 'Name': 'KNN'}\n",
            "resultes for year 2019 with model type (Word Embending)\n",
            "{'Precision': 0.48291858521340564, 'Recall': 0.5263166836561333, 'F_Measure': 0.41786711654585734, 'Accuracy': 0.5263166836561333, 'Name': 'SVM'}\n",
            "{'Precision': 0.6380095238192507, 'Recall': 0.6396024464831804, 'F_Measure': 0.6147608969639954, 'Accuracy': 0.6239381583418281, 'Name': 'Random Forest'}\n",
            "{'Precision': 0.6022901666218807, 'Recall': 0.5788226299694189, 'F_Measure': 0.5527908995719104, 'Accuracy': 0.5788226299694189, 'Name': 'KNN'}\n",
            "------------------------------------\n",
            "resultes for year 2023 with model type (tfidf)\n",
            "{'Precision': 0.7521788375583643, 'Recall': 0.7553116769095698, 'F_Measure': 0.7399329336835564, 'Accuracy': 0.7553116769095698, 'Name': 'SVM'}\n",
            "{'Precision': 0.7554367098563467, 'Recall': 0.7406496927129063, 'F_Measure': 0.7174335828351853, 'Accuracy': 0.7464881474978051, 'Name': 'Random Forest'}\n",
            "{'Precision': 0.23009937343373324, 'Recall': 0.29056189640035124, 'F_Measure': 0.13641732515263663, 'Accuracy': 0.29056189640035124, 'Name': 'KNN'}\n",
            "resultes for year 2023 with model type (Word Embending)\n",
            "{'Precision': 0.6059860691261784, 'Recall': 0.5872036874451274, 'F_Measure': 0.5115519066965499, 'Accuracy': 0.5872036874451274, 'Name': 'SVM'}\n",
            "{'Precision': 0.6591857790871313, 'Recall': 0.6535776997366111, 'F_Measure': 0.6270543067631789, 'Accuracy': 0.6551141352063213, 'Name': 'Random Forest'}\n",
            "{'Precision': 0.6435579503406557, 'Recall': 0.610820895522388, 'F_Measure': 0.5700548277024297, 'Accuracy': 0.610820895522388, 'Name': 'KNN'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, pairwise, precision_score, recall_score, f1_score, confusion_matrix, make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#upologizmos toy cros validation\n",
        "def cross_validation(model_type, year):\n",
        "\n",
        "    if model_type == \"tfidf\": #take the variables base on the model\n",
        "        X_train, X_test, y_train, y_test = TFIDF(year)\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = Word_Embeddings(year)\n",
        "\n",
        "    models = {\n",
        "        'SVM': SVC(),\n",
        "        'Random Forest': RandomForestClassifier(),\n",
        "        'KNN': KNeighborsClassifier()\n",
        "    }\n",
        "\n",
        "    print(f\"resultes for year {year} with model type ({model_type})\")\n",
        "\n",
        "    for name, model in models.items():\n",
        "        # Compute all scores for Cross Validation\n",
        "        metrics = {\n",
        "            'Precision': make_scorer(precision_score, pos_label=1, average='weighted', zero_division=0),\n",
        "            'Recall': make_scorer(recall_score, pos_label=1, average='weighted', zero_division=0),\n",
        "            'F_Measure': make_scorer(f1_score, pos_label=1, average='weighted', zero_division=0),\n",
        "            'Accuracy': 'accuracy'\n",
        "        }\n",
        "        scores = {metric: float(np.mean(cross_val_score(model, X_train, y_train, cv=10, scoring=scorer))) for metric, scorer in metrics.items()}\n",
        "        scores[\"Name\"] = name\n",
        "        print(scores)\n",
        "\n",
        "#cross validation gia 2019,2023 kai me tous 2 tropos TF-IDF,Word Embendings\n",
        "cross_validation(\"tfidf\",2019)\n",
        "cross_validation(\"Word Embending\",2019)\n",
        "print(\"------------------------------------\")\n",
        "cross_validation(\"tfidf\",2023)\n",
        "cross_validation(\"Word Embending\",2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwIXX0tXp2Vh"
      },
      "source": [
        "    \n",
        "\n",
        "* TF-IDF\n",
        "    * Αξιολόγηση\n",
        "        * SVM:Το SVM έχει την καλύτερη συνολική απόδοση από όλα τα μοντέλα. Έχει τον υψηλότερο βαθμό ακρίβειας (Precision) και ανάκλησης (Recall), καθώς και τη μεγαλύτερη ακρίβεια (Accuracy). Αυτό σημαίνει ότι το SVM είναι το πιο αξιόπιστο και αποτελεσματικό μοντέλο σε αυτή την περίπτωση.\n",
        "        * Random Forest:Το Random Forest έρχεται δεύτερο σε απόδοση. Ενώ οι τιμές του (Precision) και (Recall) είναι αρκετά κοντά σε αυτές του SVM, η συνολική ακρίβειά του είναι ελαφρώς χαμηλότερη.\n",
        "        * KNN:Το KNN έχει τη χαμηλότερη απόδοση από όλα τα μοντέλα. Οι τιμές του (Precision) και (Recall) είναι πολύ χαμηλές, όπως και η ακρίβειά του. Αυτό δείχνει ότι το KNN δεν είναι κατάλληλο για την συγκεκριμένη εργασία.\n",
        "    * Συμπεράσματα\n",
        "        * Καλύτερο Μοντέλο: Το SVM είναι ξεκάθαρα το καλύτερο μοντέλο για τα δεδομένα αυτά, με την υψηλότερη ακρίβεια και τις καλύτερες τιμές (Precision) και (Recall).\n",
        "        * Εναλλακτική Επιλογή: Το Random Forest είναι μια καλή εναλλακτική επιλογή, εάν για κάποιο λόγο δεν μπορεί να χρησιμοποιηθεί το SVM.\n",
        "        * Ανεπαρκές Μοντέλο: Το KNN δεν αποδίδει καλά και δεν είναι κατάλληλο για αυτή την εργασία.\n",
        "        * Επομένως, για τα δεδομένα που έχουμε, το SVM είναι η προτιμώμενη επιλογή λόγω της ανώτερης απόδοσής του.\n",
        "* Word Empbendings\n",
        "    * Αξιολόγηση\n",
        "        * SVM: Το SVM έχει τη χαμηλότερη απόδοση από τα τρία μοντέλα. Αν και έχει σχετικά καλό Recall, to (Precision) και η (F-Measure) του είναι χαμηλές. Αυτό υποδηλώνει ότι το μοντέλο μπορεί να εντοπίζει ένα καλό ποσοστό από τα σωστά παραδείγματα, αλλά έχει πολλά ψευδώς θετικά.\n",
        "        * Random Forest:Το Random Forest αποδεικνύεται ως το καλύτερο μοντέλο. Έχει την υψηλότερη (Precision), (Recall), (F-Measure) και (Accuracy), γεγονός που δείχνει μια καλή ισορροπία μεταξύ της ακρίβειας και της δυνατότητας ανίχνευσης σωστών παραδειγμάτων.\n",
        "        * KNN:Το KNN έχει καλύτερη απόδοση από το SVM, αλλά υπολείπεται σε σχέση με το Random Forest. To (Precision) και το (Recall) του είναι χαμηλότερα από του Random Forest, γεγονός που υποδηλώνει ότι το μοντέλο αυτό έχει μια μέτρια ικανότητα να εντοπίζει και να κατηγοριοποιεί σωστά τα παραδείγματα.\n",
        "    * Συμπεράσματα\n",
        "        * Καλύτερο Μοντέλο:Το Random Forest είναι το καλύτερο μοντέλο, με τις υψηλότερες τιμές (Precision), (Recall), (F-Measure) και (Accuracy). Είναι το πιο αξιόπιστο μοντέλο, που προσφέρει την καλύτερη ισορροπία μεταξύ ακρίβειας και ανίχνευσης σωστών παραδειγμάτων.\n",
        "        * Εναλλακτική Επιλογή:Το KNN είναι μια αποδεκτή εναλλακτική λύση, αν και δεν είναι τόσο αποτελεσματικό όσο το Random Forest.\n",
        "        * Ανεπαρκές Μοντέλο:Το SVM παρουσιάζει τη χαμηλότερη απόδοση και δεν είναι κατάλληλο για αυτή την εργασία σε σύγκριση με τα άλλα δύο μοντέλα.\n",
        "        * Επομένως, για τα δεδομένα της εργασίας, το Random Forest είναι η προτιμώμενη επιλογή λόγω της ανώτερης απόδοσής του σε όλες τις μετρικές.\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiGkCBnNu_1H",
        "outputId": "a063588e-651a-4061-e495-f9097b3cb644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['apartment', 'great', 'place', 'us', 'stay', 'athens', 'location', 'host', 'good', 'nice', 'clean', 'would', 'everything', 'close', 'really', 'also', 'night', 'one', 'metro', 'time', 'acropolis', 'recommend', 'well', 'even', 'walk', 'get', 'flat', 'perfect', 'area', 'helpful', 'room', 'need', 'comfortable', 'restaurants', 'airbnb', 'located', 'house', 'city', 'view', 'like', 'could', 'station', 'shower', 'walking', 'around', 'definitely', 'amazing', 'bathroom', 'minutes', 'easy', 'back', 'bit', 'home', 'didnt', 'day', 'little', 'made', 'kitchen', 'small', 'water', 'days', 'people', 'bed', 'lot', 'building', 'highly', 'two', 'check', 'next', 'neighborhood', 'street', 'distance', 'center', 'away', 'spacious', 'airport', 'arrival', 'experience', 'noise', 'super', 'stayed', 'much', 'kind', 'thanks', 'best', 'staying', 'balcony', 'use', 'go', 'noisy', 'find', 'space', 'near', 'communication', 'wonderful', 'lovely', 'door', 'gave', 'central', 'first', 'main', 'problem', 'needed', 'quite', 'always', 'late', 'friendly', 'left', 'hosts', 'got', 'arrived', 'excellent', 'thank', 'want', 'big', 'quiet', 'however', 'way', 'floor', 'enjoyed', 'many', 'wifi', 'see', 'sure', 'pictures', 'minute', 'make', 'dont', 'feel', 'every', 'felt', 'bad', 'found', 'money', 'places', 'help', 'morning', 'extremely', 'right', 'attractions', 'things', 'trip', 'provided', 'quick', 'price', 'take', 'sleep', 'outside', 'accommodating', 'appartment', 'met', 'overall', 'bars', 'local', 'never', 'hot', 'short', 'come', 'far', 'reservation', 'meet', 'within', 'went', 'though', 'beautiful', 'recommended', 'nights', 'better', 'centre', 'exactly', 'questions', 'cold', 'bedroom', 'thing', 'problems', 'plaka', 'shops', 'looking', 'posting', 'coffee', 'know', 'let', 'without', 'fantastic', 'public', 'canceled', 'automated', 'asked', 'air', 'enough', 'towels', 'taxi', 'came', 'youre', 'visit', 'living', 'unfortunately', 'bus', 'loud', 'available', 'nearby', 'told', 'value', 'modern', 'family', 'old', 'another', 'ever', 'breakfast', 'issues', 'say', 'everywhere', 'equipped', 'tips', 'anything', 'leave', 'new', 'eat', 'hours', 'lots', 'anyone', 'wasnt', 'someone', 'think', 'neighbourhood', 'still', 'due', 'dirty', 'food', 'studio', 'min', 'hard', 'ok', 'toilet', 'cool', 'greek', 'took', 'booked', 'windows', 'described', 'might', 'large', 'convenient', 'awesome', 'tourist', 'loved', 'open', 'pleasant', 'couldnt', 'said', 'flight', 'checked', 'beds', 'cant', 'full', 'welcoming', 'easily', 'early', 'transportation', 'etc', 'inside', 'light', 'person', 'going', 'issue', 'safe', 'last', 'square', 'welcome', 'train', 'absolutely', 'since', 'access', 'roof', 'terrace', 'hospitality', 'book', 'checkin', 'making', 'long', 'second', 'return', 'keys', 'especially', 'guests', 'hour', 'reach', 'cheap', 'views', 'expected', 'able', 'cozy', 'broken', 'nothing', 'although', 'greece', 'work', 'directions', 'difficult', 'couple']\n"
          ]
        }
      ],
      "source": [
        "#Ερώτημα 3) (Mono gia to 2019)\n",
        "#Ebresei ton (number) pio emfanizomenon lekseon gia ta sxoleia mias xronias.Ypologizmos tou Cosine Similarity gia autes tiw lekseis (for year 2019 only).Kai exoyme kai mia sunartisi poy mas epistrefei\n",
        "#mia lista me tis pio similar lekseis gia mia leksi (geitonia)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def Most_Popular(number = 300):\n",
        "    data_tsv = load_tsv(2019) #load TSV\n",
        "\n",
        "    most_popular_voc = {}\n",
        "\n",
        "    #upologismos tis sunxotitas emfanisis kathe lejeis\n",
        "    for comment in data_tsv[\"comments\"]:\n",
        "        for word in word_tokenize(comment):\n",
        "            if word not in most_popular_voc:\n",
        "                most_popular_voc[word] = 1\n",
        "            else:\n",
        "                most_popular_voc[word] += 1\n",
        "\n",
        "    #sortarisma ton lejeon simfona me thn emfanosh tous kai apothikeusei ton (number) kaliteron\n",
        "    most_popular_voc = sorted(most_popular_voc.items(), key=lambda item: item[1],reverse=True)\n",
        "    most_popular_voc = dict(most_popular_voc[:number])\n",
        "\n",
        "    #epistrofi ton pio suxnon + train to modelo me ola ta dedomena\n",
        "    return list(most_popular_voc.keys()), Word2Vec(sentences=[word_tokenize(comment) for comment in data_tsv['comments']], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def Compute_CosSim(most_popular_words,model): #cosine similarity for the Most Populat Words for the year we want\n",
        "\n",
        "    vectorizer = [model.wv[word] for word in most_popular_words] #briskoume ta vectors gia kathe dimofili leksei\n",
        "    vectorizer_array = np.array(vectorizer)\n",
        "    similarity_array = cosine_similarity(vectorizer_array) #briskoyme to cosine similarity gia tis pio dimofilis lejeis\n",
        "\n",
        "    return similarity_array\n",
        "\n",
        "def Compute_Neigborhood(word, word_list, N, model):\n",
        "    similar_words = []\n",
        "    for w in word_list: #gia kathe dimofili lejei\n",
        "        if w != word:   #amma den einai o eautos mas\n",
        "            similarity = model.wv.similarity(word, w) #upologizoyme to similarity score\n",
        "            similar_words.append((w, similarity))\n",
        "    similar_words.sort(key=lambda x: x[1], reverse=True) #kanoyme sort tis lejeis simfona me to score\n",
        "\n",
        "    return [w[0] for w in similar_words[:N]] #pernoume tis N protes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "most_popular_words, model = Most_Popular()\n",
        "\n",
        "similarity_array = Compute_CosSim(most_popular_words, model)\n",
        "\n",
        "print(most_popular_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrJlc0ReALyV"
      },
      "outputs": [],
      "source": [
        "#Ypologizoyme tis 3 sinartisis (1)Maximum Similarity of Neighborhoods (2)Correlation of Neighborhood Similarities (3)Sum of Squared Neighborhood Similarities\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "import math\n",
        "import random\n",
        "\n",
        "#a)Maximum Similarity of Neighborhoods\n",
        "def max_sim_of_Neig(word1,word2,neig1,neig2,similarity_array,most_popular_words):\n",
        "    #Mn(wi,wj) = max{αij,αji}\n",
        "    sim1 = [similarity_array[most_popular_words.index(word1)][most_popular_words.index(w2)] for w2 in neig2] #S(wi,x) x ∈ Nj\n",
        "    sim2 = [similarity_array[most_popular_words.index(word2)][most_popular_words.index(w1)] for w1 in neig1] #S(wj,y) y ∈ Ni\n",
        "\n",
        "    max1 = sorted(sim1,reverse=True)[0] #αij = max S(wi,x) x ∈ Nj\n",
        "    max2 = sorted(sim2,reverse=True)[0] #αij = max S(wj,y) y ∈ Ni\n",
        "\n",
        "    return max(max1,max2) #max{αij , αji}\n",
        "\n",
        "#b)Correlation of Neighborhood Similarities\n",
        "def correlation_of_neighborhood(word1, word2, neig1, neig2, similarity_array, most_popular_words):\n",
        "    #Rn(wi,wj) = max{βij,βji}\n",
        "    same_word_neigh1 = [similarity_array[most_popular_words.index(word1)][most_popular_words.index(w1)] for w1 in neig1] #Ci στο Ni\n",
        "    same_word_neigh2 = [similarity_array[most_popular_words.index(word2)][most_popular_words.index(w2)] for w2 in neig2] #Cj στο Nj\n",
        "\n",
        "    different_word_neigh1 = [similarity_array[most_popular_words.index(word1)][most_popular_words.index(w2)] for w2 in neig2] #Ci στο Nj\n",
        "    different_word_neigh2 = [similarity_array[most_popular_words.index(word2)][most_popular_words.index(w1)] for w1 in neig1] #Cj στο Ni\n",
        "\n",
        "    correlation_coefficient1 = np.corrcoef(same_word_neigh1, different_word_neigh2)[0, 1] #ρ(Ci στο Ni,Cj στο Ni) βij\n",
        "    correlation_coefficient2 = np.corrcoef(different_word_neigh1, same_word_neigh2)[0, 1] #ρ(Ci στο Nj,Cj στο Nj) βji\n",
        "\n",
        "    return max(correlation_coefficient1,correlation_coefficient2) #max{βij,βji}\n",
        "\n",
        "#c)Sum of Squared Neighborhood Similarities\n",
        "def Sum_of_Sq_Neig_Sim(word1,word2,neig1,neig2,similarity_array,most_popular_words):\n",
        "    # θ             θ           θ       1/θ\n",
        "    #E(wi,wj) = (Σ S(wi,x) + Σ S(wj,y))\n",
        "    # n        x ∈ Nj      y ∈ Ni\n",
        "                                                                                                                  # 2\n",
        "    sim1 = [similarity_array[most_popular_words.index(word1)][most_popular_words.index(w2)] ** 2 for w2 in neig2] #S(wi,x) x ∈ Nj\n",
        "                                                                                                                  # 2\n",
        "    sim2 = [similarity_array[most_popular_words.index(word2)][most_popular_words.index(w1)] ** 2 for w1 in neig1] #S(wj,y) y ∈ Ni\n",
        "\n",
        "    sum1 = sum(sim1) #Σ sim1\n",
        "    sum2 = sum(sim2) #Σ sim2\n",
        "\n",
        "    Total_sum = sum1 + sum2\n",
        "    return math.sqrt(Total_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayq0wEBhk6oC",
        "outputId": "8021f41e-ba3b-4d61-ca2c-9615765bd3d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "For the N=2 neighborhood\n",
            "neighborhood words for 'extremely' is ['also', 'apartment']\n",
            "neighborhood words for 'would' is ['place', 'athens']\n",
            "-------------------------------------------\n",
            "Maximum Similarity of Neighborhoods 0.99973464012146\n",
            "Correlation of Neighborhood Similarities 1.0\n",
            "Sum of Squared Neighborhood Similarities 1.9993695627690278\n",
            "-------------------------------------------\n",
            "For the N=10 neighborhood\n",
            "neighborhood words for 'extremely' is ['also', 'apartment', 'door', 'people', 'day', 'airbnb', 'good', 'get', 'time', 'little']\n",
            "neighborhood words for 'would' is ['place', 'athens', 'made', 'time', 'room', 'apartment', 'check', 'go', 'stay', 'two']\n",
            "-------------------------------------------\n",
            "Maximum Similarity of Neighborhoods 0.9997454285621643\n",
            "Correlation of Neighborhood Similarities 0.3744405972876843\n",
            "Sum of Squared Neighborhood Similarities 4.470609478198876\n",
            "-------------------------------------------\n",
            "For the N=30 neighborhood\n",
            "neighborhood words for 'extremely' is ['also', 'apartment', 'door', 'people', 'day', 'airbnb', 'good', 'get', 'time', 'little', 'place', 'made', 'one', 'find', 'flat', 'night', 'way', 'even', 'athens', 'building', 'water', 'room', 'house', 'like', 'use', 'didnt', 'everything', 'could', 'us', 'floor']\n",
            "neighborhood words for 'would' is ['place', 'athens', 'made', 'time', 'room', 'apartment', 'check', 'go', 'stay', 'two', 'one', 'house', 'people', 'use', 'bit', 'get', 'water', 'us', 'also', 'staying', 'lot', 'neighborhood', 'much', 'back', 'airbnb', 'like', 'left', 'recommend', 'could', 'building']\n",
            "-------------------------------------------\n",
            "Maximum Similarity of Neighborhoods 0.9997825622558594\n",
            "Correlation of Neighborhood Similarities 0.2914238284306096\n",
            "Sum of Squared Neighborhood Similarities 7.743269072955351\n",
            "\n",
            "\n",
            "Your Example \n",
            "-------------------------------------------\n",
            "For the N=10 neighborhood\n",
            "neighborhood words for 'posting' is ['reservation', 'canceled', 'automated', 'arrival', 'days', 'host', 'accommodating', 'living', 'booked', 'long']\n",
            "neighborhood words for 'hours' is ['didnt', 'could', 'things', 'room', 'helpful', 'host', 'area', 'like', 'even', 'us']\n",
            "-------------------------------------------\n",
            "Maximum Similarity of Neighborhoods 0.9995781779289246\n",
            "Correlation of Neighborhood Similarities 0.10829392467727934\n",
            "Sum of Squared Neighborhood Similarities 4.413969479981834\n"
          ]
        }
      ],
      "source": [
        "#Sunartisi (Valid_Word) poy elenxei an mia lejei einai stis Most Popular,an oxi dialegei mia allh mesa apo autes tixea + elenxos estei oste na min einai kai oi 2 lejeis oi idies (poy sigkrinoyme)\n",
        "#Sunartisi (Run_All_Similarity_Fun) pou upologizei tis geitonies shmfona me to N gia ta valid words ektiposi ton geitonion + ektiposi kai upologizmos ton Similarity me tous 3 diaforetikoys tropous\n",
        "\n",
        "def Valid_Word(word1,word2,most_popular_words):\n",
        "    if word1 not in most_popular_words: #if first word no in Popupal words\n",
        "        word1 = random.choice(most_popular_words) #take a random value from the most Popupal words\n",
        "\n",
        "    while word1 == word2: #in case both words is the same\n",
        "        word1 = random.choice(most_popular_words)\n",
        "\n",
        "    return word1\n",
        "\n",
        "def Run_All_Similarity_Fun(word1,word2,N,similarity_array,most_popular_words):\n",
        "    #check if example words is valid,if not take a valid one\n",
        "    word1 = Valid_Word(word1,word2,most_popular_words)\n",
        "    word2 = Valid_Word(word2,word1,most_popular_words)\n",
        "\n",
        "    #compute neighborhood for the 2 words\n",
        "    neig1 = Compute_Neigborhood(word1,most_popular_words,N,model)\n",
        "    neig2 = Compute_Neigborhood(word2,most_popular_words,N,model)\n",
        "\n",
        "    #print the neighborhood for each word in example\n",
        "    print(\"-------------------------------------------\")\n",
        "    print(f\"For the N={N} neighborhood\")\n",
        "    print(f\"neighborhood words for '{word1}' is {neig1}\")\n",
        "    print(f\"neighborhood words for '{word2}' is {neig2}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "\n",
        "    #compute the similarity with all the functions\n",
        "    Maximum_Similarity_of_Neighborhoods = max_sim_of_Neig(word1,word2,neig1,neig2,similarity_array,most_popular_words)\n",
        "    Correlation_of_Neighborhood_Similarities = correlation_of_neighborhood(word1, word2, neig1, neig2, similarity_array, most_popular_words)\n",
        "    Sum_of_Squared_Neighborhood_Similarities = Sum_of_Sq_Neig_Sim(word1,word2,neig1,neig2,similarity_array,most_popular_words)\n",
        "\n",
        "    print(f\"Maximum Similarity of Neighborhoods {Maximum_Similarity_of_Neighborhoods}\")\n",
        "    print(f\"Correlation of Neighborhood Similarities {Correlation_of_Neighborhood_Similarities}\")\n",
        "    print(f\"Sum of Squared Neighborhood Similarities {Sum_of_Squared_Neighborhood_Similarities}\")\n",
        "\n",
        "word1 = \"xxxxx\"\n",
        "word2 = \"yyyyy\"\n",
        "\n",
        "word1 = Valid_Word(word1,word2,most_popular_words)\n",
        "word2 = Valid_Word(word2,word1,most_popular_words)\n",
        "\n",
        "#example to see the diference between similarity base on diferent N(number of neighborhood)\n",
        "Run_All_Similarity_Fun(\"extremely\",\"would\",2,similarity_array,most_popular_words)\n",
        "Run_All_Similarity_Fun(\"extremely\",\"would\",10,similarity_array,most_popular_words)\n",
        "Run_All_Similarity_Fun(\"extremely\",\"would\",30,similarity_array,most_popular_words)\n",
        "\n",
        "\n",
        "#example that you can change whatever you want between (----------) Your example\n",
        "#---------------------------------+\n",
        "N = 10 #number of neighborhood  plz give above 1 [2,whatever you want]\n",
        "word1 = 'Kalispera'\n",
        "word2 = 'Holla'\n",
        "#---------------------------------+\n",
        "print(\"\\n\\nYour Example \")\n",
        "Run_All_Similarity_Fun(word1,word2,N,similarity_array,most_popular_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_yBzoWprh9l"
      },
      "source": [
        "Συμφονα με το παραπανω παραδιγμα μπορουμε να κανουμε τις εξεις παρατιρισεις σχετικα με την αλλαγη των τιμων Similarity συμφονα με το N.\n",
        "\n",
        "* Χαμιλο Ν (πχ Ν = 2)\n",
        "    * Sum of Squared Neighborhood Similarities:Μπορουμε να δουμε οτι οσο χαμιλοτερο ειναι το Ν τοσο λιγοτερο ειναι και το Sum of Squared Neighborhood Similarities.\n",
        "    * Maximum Similarity of Neighborhoods: Μπορουμε να δουμε οτι οσο χαμιλο και αν ειναι το Ν το Maximum Similarity of Neighborhoods παραμενει σχεδον ανεπαφο.\n",
        "* Υψυλα Ν (πχ Ν = 30)\n",
        "    * Sum of Squared Neighborhood Similarities: Μπορουμε να δουμε οτι οσο υψιλοτερ ειναι το Ν τοσο μεγαλυτερο ειναι και το Sum of Squared Neighborhood Similarities.\n",
        "    * Maximum Similarity of Neighborhoods: Μπορουμε να δουμε οτι οσο μεγαλο και αν ειναι το Ν το Maximum Similarity of Neighborhoods παραμενει σχεδον ανεπαφο.\n",
        "\n",
        "* Οσο και αν ειναι το Ν (πχ Ν=2 ή Ν=10 ή Ν=30) δεν μπορουμε να βγαλουμε καποιο πορισμα σχετικα με το Correlation of Neighborhood Similarities και την εξαρτιση που εχει με το Ν μιας και φενετε να μην επιρεαζετε με καποιο τροπο απο αυτο. Δεν φενετε στα αποτελεσματα του να υπαρχει καποιο εμφανης μοτιβο"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
